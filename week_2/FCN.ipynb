{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import VOCLoader  # import the custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = VOCLoader.load(train_batch_size=32, test_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = VOCSegmentation(root=\"./data\", year='2012', image_set='train')\n",
    "\n",
    "# images, masks = train_dataset[0]\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# ax[0].imshow(images)\n",
    "# ax[1].imshow(masks)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "images, masks = next(iter(train_loader))\n",
    "\n",
    "# show masks\n",
    "VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
    "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
    "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
    "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
    "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
    "                [0, 64, 128]]\n",
    "\n",
    "def mask2label(mask, num_classes=21):\n",
    "    mask = mask.numpy()  # (1, 256, 256)\n",
    "    mask = mask[0]  # remove the channel dimension\n",
    "    mask = np.uint8(mask * 255)  # convert to 0-255 range\n",
    "    \n",
    "    label_colors = np.array(VOC_COLORMAP)\n",
    "    r = np.zeros_like(mask).astype(np.uint8)\n",
    "    g = np.zeros_like(mask).astype(np.uint8)\n",
    "    b = np.zeros_like(mask).astype(np.uint8)\n",
    "    \n",
    "    for l in range(0, num_classes):\n",
    "        idx = mask == l\n",
    "        r[idx] = label_colors[l, 0]\n",
    "        g[idx] = label_colors[l, 1]\n",
    "        b[idx] = label_colors[l, 2]\n",
    "    \n",
    "    # border\n",
    "    idx = mask == 255\n",
    "    r[idx] = 255\n",
    "    g[idx] = 255\n",
    "    b[idx] = 255\n",
    "    \n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "    rgb = transforms.ToTensor()(rgb)\n",
    "    return rgb\n",
    "\n",
    "\n",
    "num_images = 4\n",
    "images = images[:num_images]\n",
    "masks = masks[:num_images]\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# show images\n",
    "for i, image in enumerate(images):\n",
    "    # Reverse of the transformation used in the dataloader\n",
    "    image[0] = image[0] * 0.229 + 0.485\n",
    "    image[1] = image[1] * 0.224 + 0.456\n",
    "    image[2] = image[2] * 0.225 + 0.406\n",
    "    npimg = image.numpy()\n",
    "    plt.subplot(2, num_images, i + 1)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "# show labels\n",
    "labels = map(mask2label, masks)\n",
    "for i, label in enumerate(labels):\n",
    "    plt.subplot(2, num_images, i + 1 + num_images)\n",
    "    plt.imshow(label.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN\n",
    "See https://arxiv.org/abs/1411.4038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
    "          torch.arange(kernel_size).reshape(1, -1))\n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
    "           (1 - torch.abs(og[1] - center) / factor)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "                          kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FCN model\n",
    "# class FCN(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(FCN, self).__init__()\n",
    "#         # Load the pre-trained VGG16 model\n",
    "#         resnet18 = models.resnet18(pretrained=True)\n",
    "#         features = list(resnet18.features.children())\n",
    "#         self.features = nn.Sequential(*features[:-2])  # Extract features until the last max pooling layer\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Conv2d(512, num_classes, kernel_size=1),\n",
    "#             nn.ConvTranspose2d(num_classes, num_classes,\n",
    "#                                 kernel_size=64, padding=16, stride=32)\n",
    "#         )\n",
    "            \n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "FCN = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "\n",
    "num_classes = 21  # Pascal VOC dataset has 21 classes\n",
    "FCN.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n",
    "FCN.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n",
    "                                    kernel_size=64, padding=16, stride=32))\n",
    "\n",
    "W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "FCN.transpose_conv.weight.data.copy_(W);\n",
    "\n",
    "# Initialize FCN model\n",
    "# model = FCN(num_classes)\n",
    "model = FCN\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = lambda inputs, targets: F.cross_entropy(inputs, targets, reduction='none', ignore_index=255).mean(1).mean(1)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(size=(1, 3, 320, 480))\n",
    "FCN(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "num_epochs = 5\n",
    "total_step = len(train_loader)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, masks) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        labels = (masks.squeeze(1) * 255).long()\n",
    "        loss = criterion(outputs, labels).mean()  # Average mean loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())  # Save the loss\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'fcn_resnet18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "# Plot vertical lines at the end of each epoch\n",
    "for i in range(num_epochs):\n",
    "    plt.axvline(x=total_step*(i+1), color='r', linestyle='--')\n",
    "    \n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model.load_state_dict(torch.load('fcn_resnet18.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # for images, labels in test_loader:\n",
    "    #     output = model(images)\n",
    "    #     _, predicted = torch.max(output, 1)\n",
    "        \n",
    "    #     # Visualize input image, ground truth, and predicted segmentation mask\n",
    "    #     plt.figure(figsize=(10, 5))\n",
    "    #     plt.subplot(1, 3, 1)\n",
    "    #     plt.title(\"Input Image\")\n",
    "    #     plt.imshow(transforms.ToPILImage()(images.squeeze()))\n",
    "    #     plt.axis('off')\n",
    "        \n",
    "    #     plt.subplot(1, 3, 2)\n",
    "    #     plt.title(\"Ground Truth\")\n",
    "    #     plt.imshow(transforms.ToPILImage()(mask2label(labels.squeeze())))\n",
    "    #     plt.axis('off')\n",
    "        \n",
    "    #     plt.subplot(1, 3, 3)\n",
    "    #     plt.title(\"Predicted\")\n",
    "    #     plt.imshow(transforms.ToPILImage()(mask2label(predicted.squeeze())))\n",
    "    #     plt.axis('off')\n",
    "        \n",
    "    #     plt.show()\n",
    "    #     break  # Show only one example for brevity\n",
    "    \n",
    "    num_images = 4\n",
    "    images, masks = next(iter(train_loader))\n",
    "    images = images[:num_images]\n",
    "    masks = masks[:num_images]\n",
    "    output = model(images)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted = predicted.unsqueeze(1)  # Add the channel dimension\n",
    "    \n",
    "    # show images\n",
    "    for i, image in enumerate(images):\n",
    "        # Reverse of the transformation used in the dataloader\n",
    "        image[0] = image[0] * 0.229 + 0.485\n",
    "        image[1] = image[1] * 0.224 + 0.456\n",
    "        image[2] = image[2] * 0.225 + 0.406\n",
    "        npimg = image.numpy()\n",
    "        plt.subplot(3, num_images, i + 1)\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        plt.axis('off')\n",
    "\n",
    "    # show ground truth\n",
    "    labels = map(mask2label, masks)\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.subplot(3, num_images, i + 1 + num_images)\n",
    "        plt.imshow(label.permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        \n",
    "    # show predictions\n",
    "    predictions = map(mask2label, predicted)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        plt.subplot(3, num_images, i + 1 + 2*num_images)\n",
    "        plt.imshow(prediction.permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
